{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duianqDQ7NWB"
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbT3BX0y7VGt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338,
     "referenced_widgets": [
      "2ed44f95bf54451bb93d482be47de0c6",
      "78c9bb2f76be402ca59f86d432f3e00c",
      "bff18fb6e4754f53999983115bda99f5",
      "855af66e071e4888ae2c701d9f51fbcd",
      "5597f5dfcf334eb5bf279214c6a1526e",
      "617e8ef5c55f4496ba858074443d0b9d",
      "150fc16e3ae044a2806bfa612d55f6d3",
      "b55926e92afb47d182addb84c1b938d3",
      "dd86ff2c1d3f4b6796aadaf1f9b8b6b8",
      "45b4c4cc2b5a471396186136916e4cb8",
      "5567fbd0bf354b23a7824db4f4c63d72",
      "4e1efac10aa749de98add6df55d8e6d8",
      "9b798193a1a345deb1948c07f4add255",
      "7ee3ca8d83394399959484c7cc033861",
      "ba720921665b403dbeadf3f3dfa69fae",
      "826fe87b680246828b5b72e26fceb65a",
      "02fdad02bc394da399bfbdf22688a670",
      "bf2d8786e9414eaa872891b7a0f6b87a",
      "53aebdc52e654259afed0beb19339f38",
      "14552ec65c904c8299c790f3b3cf1989",
      "960b4ef8566f410f892a6a3c3d769cf6",
      "29231ac27a3b4a1d9b9976e74b117261",
      "459fb2bf24a34f64a353abd092a2582c",
      "2353cf5f310d450781cb2896e9d37ce4",
      "99da53feb83a46c0b86c43b663727010",
      "8873f677a7f64e6283adb62c9ce50d07",
      "4ff467397e954b4a80f2ce28e9cd5710",
      "c86bee410cea4543b748d56ef5714281",
      "0a810cb72f34431a885049d20947149c",
      "03c55ce0f6734385bc3a087ffae17c62",
      "9f04cbe0c16541a5b2b4e012b0592726",
      "53cd3207ed424387b1d22cfa77e67d9a",
      "e4edb7324ac54df987b3e2e7096ac4d8",
      "e93764dea1b74d94b55fcdfbe17b44bb",
      "64b0726a426f4311a77295d522989b8c",
      "1bbc9eb60ed4406d8708e39d105d0fd9",
      "e4b1e7489ca24deeabcb687a1dfb698e",
      "3a26da8260c44c3f9dc53f1727ece606",
      "4f0fb61c19224763896624bd09bb9856",
      "173db3a19cda40909b2376efce84b6f9",
      "81380b6ef3f54d2c9a05eed2b5d20ce9",
      "cf6b6d6ccf044e00853a7ec3afdfbb47",
      "32d9cb8481864e2ab3dd3cd53236403a",
      "ad4962defb1d44558c144459696682f0",
      "79eb14d8805c4d58a61e4c1c997f1cfe",
      "0314ab4b5c80443bb8a1aa4cce76582a",
      "7ed34db36a2d47d0b579f62f7e58f8a0",
      "8b2330faea67411b8aa09d9a661bc8a1",
      "de6424130d8343c0a28e7d36ceb3975a",
      "dfb09a78944041c6a6b1420e336d0650",
      "3d62f2d22d6e48ff983f006563995ba8",
      "8024c213f4ca43d2b3633cbd4954fdd4",
      "1d97b7cdfb854c819262e72cac5ace58",
      "d030b26d0e1b4c469442262541eb91ed",
      "f7080d5a79234628b7ee567e200e54ac",
      "dc3a37922fed431a8f6d3e8b475873d6",
      "4d0eb7f0b38940208d1ab95f20c70784",
      "172778dac6fb464fa953e99c4a562ab7",
      "783e11651ddd4d3e9a70ceab3bb03dfb",
      "3379e2bc1c104a5680fefa6afafe03d9",
      "97f7d0cea96849ec894edcd943e9fd04",
      "c271912402674f8aad65d109bb181396",
      "39abd63802634af9b4d0c47224dd8c23",
      "fddfc543f2024b9e977707ac3d946e34",
      "37e5288a7bf64d798a2a27f7d560d62b",
      "16cd364e579a41f69d2daea2cc64de8d"
     ]
    },
    "id": "KHd20hVH7Yy8",
    "outputId": "298d8254-e899-4e42-b137-591ddd942a9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed44f95bf54451bb93d482be47de0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1efac10aa749de98add6df55d8e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459fb2bf24a34f64a353abd092a2582c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93764dea1b74d94b55fcdfbe17b44bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eb14d8805c4d58a61e4c1c997f1cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3a37922fed431a8f6d3e8b475873d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/419M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"thanaphatt1/WangchanBERTa-LST20\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"thanaphatt1/WangchanBERTa-LST20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOXzQEae7feP"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/ws_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN9H2wws71Is"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=510):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        token_length = len(tokenizer.tokenize(word))\n",
    "        if current_length + token_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(word)\n",
    "        current_length += token_length\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJUB8YJp8Cr-"
   },
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es4EbXis8ZBd"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for chunk in text_chunks:\n",
    "    # Add add_special_tokens=False\n",
    "    tokens = tokenizer(chunk, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True, max_length=512, add_special_tokens=False)\n",
    "    inputs = tokens[\"input_ids\"]\n",
    "    offsets = tokens.get(\"offset_mapping\", [[]])[0]\n",
    "\n",
    "    if inputs.shape[1] == 0 or len(offsets) == 0:\n",
    "        continue  # Skip empty inputs\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs).logits\n",
    "    predictions = torch.argmax(outputs, dim=2).squeeze().tolist()\n",
    "\n",
    "    for idx in range(len(offsets)):\n",
    "        if idx >= len(predictions):\n",
    "            continue  # Prevent index out of range\n",
    "        start, end = offsets[idx]\n",
    "        if start == 0 and end == 0:\n",
    "            continue  # Skip special tokens\n",
    "        if idx == 0 or (idx > 0 and offsets[idx - 1][1] == start):\n",
    "            label = \"B_WORD\"\n",
    "        elif idx + 1 < len(offsets) and offsets[idx + 1][0] == end:\n",
    "            label = \"I_WORD\"\n",
    "        else:\n",
    "            label = \"E_WORD\"\n",
    "        # Make sure token is in the tokenizer's vocabulary before converting\n",
    "        if inputs[0][idx].item() in tokenizer.get_vocab():\n",
    "            all_tokens.append(tokenizer.convert_ids_to_tokens(inputs[0][idx]))\n",
    "            all_labels.append(label)\n",
    "        else:\n",
    "            # Handle unknown tokens (e.g., replace with [UNK])\n",
    "            all_tokens.append(tokenizer.unk_token)\n",
    "            all_labels.append(label)  # or assign a specific label for unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmXPzJE18bTN",
    "outputId": "7bc4fc67-0f01-48d1-c579-aa01c10ff060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmentation completed. Output saved to ws_output.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Token\": all_tokens, \"Label\": all_labels})\n",
    "df.to_csv(\"/content/ws_sample_submission.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Word segmentation completed. Output saved to ws_output.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
